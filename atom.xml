<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Textstückchen]]></title>
  <link href="http://www.dbrunner.de/atom.xml" rel="self"/>
  <link href="http://www.dbrunner.de/"/>
  <updated>2015-12-20T12:26:18+00:00</updated>
  <id>http://www.dbrunner.de/</id>
  <author>
    <name><![CDATA[Daniel Brunner]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[I played with CHICKEN Scheme, Docker and Alpine Linux]]></title>
    <link href="http://www.dbrunner.de/blog/2015/12/19/i-played-with-chicken-scheme/"/>
    <updated>2015-12-19T17:54:50+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/12/19/i-played-with-chicken-scheme</id>
    <content type="html"><![CDATA[<p>I am looking forward to meet LISP people at the
<a href="https://events.ccc.de/congress/2015/wiki/Main_Page">32c3&rsquo;s</a> <a href="https://events.ccc.de/congress/2015/wiki/Assembly:The_%28un%29employed_schemers_%26_lispers_guild">LISP assembly</a>. The last days I played a bit with different Scheme
implementations including
<a href="http://call-cc.org">CHICKEN scheme</a>. The main feature of CHICKEN is
that it compiles the Scheme code to C and then creates dynamic
libraries and binaries with the C compiler. I thought that combining
these binaries with a minimal Docker container could give me a very
small deployment. So here are my steps:</p>

<h2>Choosing Alpine Linux as a &ldquo;small&rdquo; Linux</h2>

<p>The smallest Linux image for Docker is undoubtly busybox with a size
of about 2.489 MB. But busybox lacks a package manager which makes
installing software painful. Therefore I have chosen
<a href="http://alpinelinux.org">Alpine Linux</a> which comes with package
manager and it&rsquo;s image&rsquo;s size is about 5.234 MB. That&rsquo;s double the
size of the busybox image but still quite small compared to the Ubuntu
image which is about 266 MB.</p>

<h2>Creating a Docker container with CHICKEN</h2>

<p>Alpine Linux comes with the <a href="http://www.muscl-libc.org">musl libc</a> and
I thought it would be best to compile all the CHICKEN stuff with that
libc. Therefore I created a Docker container with gcc and all the
other stuff with this Dockerfile
(<a href="https://github.com/krrrcks/chicken-docker-alpine">Github repository</a>):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>FROM alpine:3.2
</span><span class='line'>
</span><span class='line'>RUN apk update && apk add make gcc musl-dev 
</span><span class='line'>RUN wget -O - http://code.call-cc.org/releases/4.10.0/chicken-4.10.0.tar.gz | tar xz
</span><span class='line'>
</span><span class='line'>WORKDIR /chicken-4.10.0
</span><span class='line'>
</span><span class='line'>RUN make PLATFORM=linux && make PLATFORM=linux install
</span><span class='line'>
</span><span class='line'>RUN rm -fr /chicken-4.10.0 
</span><span class='line'>
</span><span class='line'>WORKDIR /
</span><span class='line'>
</span><span class='line'>CMD ["csi"]</span></code></pre></td></tr></table></div></figure>


<p>This image is quite big (about 161.7 MB) and is available for download
at the <a href="https://hub.docker.com/r/krrrcks/chicken-alpine/">Docker Hub</a>.</p>

<h2>Compiling some CHICKEN code</h2>

<p>For testing purposes I wanted a minimal web server running in the Alpine
Linux image. Therefore I looked through the
<a href="http://wiki.call-cc.org/chicken-projects/egg-index-4.html">egg index</a>
and found <a href="http://wiki.call-cc.org/eggref/4/spiffy">spiffy</a>. I fired
up the <code>chicken-alpine</code> container (but I used <code>ash</code> as command instead
of the <code>csi</code> Scheme interpreter) and created a small web server that
serves some static pages. I wrote a <code>main.scm</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>(use spiffy)
</span><span class='line'>(start-server)</span></code></pre></td></tr></table></div></figure>


<p>and added some static pages to a <code>./web</code> sub-directory. Then
everything had to be compiled and prepared for deployment:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chicken-install spiffy
</span><span class='line'>csc -deploy main.scm
</span><span class='line'>chicken-install -deploy -p $PWD/main spiffy</span></code></pre></td></tr></table></div></figure>


<h2>Deploy in a fresh Alpine Linux image</h2>

<p>After the compilation I copied the <code>main</code> and <code>web</code> directories on my
host machine using <code>docker cp</code> and created the following Dockerfile:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>FROM alpine:3.2
</span><span class='line'>
</span><span class='line'>ADD main /main
</span><span class='line'>ADD web main/web
</span><span class='line'>WORKDIR main
</span><span class='line'>
</span><span class='line'>CMD /main/main</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>and let <code>docker build -t krrrcks/spiffy-test .</code> do the job. The size
of the resulting image is about 12.37 MB and that&rsquo;s pretty small. I
uploaded that image to the
<a href="https://hub.docker.com/r/krrrcks/spiffy-test/">Docker Hub</a> as well.</p>

<p>To serve the pages I did a <code>docker run -d -p 8080:8080 krrrcks/spiffy-test</code>
(spiffy listens on port 8080 in the default install) and browsed my
static pages.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to use GET Bucket location on Amazon S3 with Racket]]></title>
    <link href="http://www.dbrunner.de/blog/2015/09/04/how-to-use-getbucketlocation-on-amazon-s3-with-racket/"/>
    <updated>2015-09-04T07:23:43+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/09/04/how-to-use-getbucketlocation-on-amazon-s3-with-racket</id>
    <content type="html"><![CDATA[<p>In <a href="http://www.racket-lang.org">Racket</a> I want to iterate over my
buckets in Amazon S3. They are located in different regions. So how do
I get my bucket&rsquo;s location/region? In the API Reference there is a
call
<a href="http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlocation.html">GET Bucket location</a>. I
use
<a href="https://github.com/greghendershott/aws">Greg&rsquo;s AWS library for Racket</a>
and this library authenticates its calls with
<a href="http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-auth-using-authorization-header.html">signature version V4</a>. But
V4 requires the user to know the <em>region</em> to correctly sign the
request. So I need to know the region to ask Amazon S3 for the region
where the bucket is located. Otherwise Amazon S3 responds with an
error:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
</span><span class='line'>&lt;Error&gt;
</span><span class='line'> &lt;Code&gt;AuthorizationHeaderMalformed&lt;/Code&gt;
</span><span class='line'> &lt;Message&gt;The authorization header is malformed; the region 'us-east-1'
</span><span class='line'>is wrong; expecting 'eu-central-1'&lt;/Message&gt;
</span><span class='line'> &lt;Region&gt;eu-central-1&lt;/Region&gt;
</span><span class='line'> &lt;RequestId&gt;XXXX&lt;/RequestId&gt;
</span><span class='line'> &lt;HostId&gt;XXXX&gt;
</span><span class='line'>&lt;/Error&gt;</span></code></pre></td></tr></table></div></figure>


<p>After some search on the net I found a
<a href="http://stackoverflow.com/questions/27091816/retrieve-buckets-objects-without-knowing-buckets-region-with-aws-s3-rest-api">post on Stackoverflow</a>
that helped to solve that issue: If I use the URL format (instead of
the normally used virtual host format) I could get the location of
any bucket. Every region responds with a <em>LocationConstraint</em> answer.</p>

<p>Therefore a code snippet for Racket could be:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>(define (get-bucket-location bucket)
</span><span class='line'>  (parameterize
</span><span class='line'>      ([s3-path-requests? #t])
</span><span class='line'>    (define xpr (get/proc (string-append bucket "/?location") read-entity/xexpr))
</span><span class='line'>    (and (list? xpr)
</span><span class='line'>         (= (length xpr) 3)
</span><span class='line'>         (third xpr))))</span></code></pre></td></tr></table></div></figure>


<p>For example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; (get-bucket-location "my-bucket-somewhere")
</span><span class='line'>"eu-central-1"</span></code></pre></td></tr></table></div></figure>


<p>PS: I think official Amazon S3 documentation could be a bit more verbose on
the issues with GetBucketLocation and signature V4.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to run Racket on the Raspberry Pi 2]]></title>
    <link href="http://www.dbrunner.de/blog/2015/08/27/how-to-run-racket-on-the-raspberry-pi-2/"/>
    <updated>2015-08-27T13:25:45+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/08/27/how-to-run-racket-on-the-raspberry-pi-2</id>
    <content type="html"><![CDATA[<p>I got a
<a href="https://www.raspberrypi.org/products/raspberry-pi-2-model-b/">Raspberry Pi 2 Model B</a>
to play with. I used Raspbian image as operating system.  I was
wondering how difficult it is to get Racket running on the Raspberry
Pi. I downloaded the
<a href="http://mirror.racket-lang.org/installers/6.2.1/racket-6.2.1-src-builtpkgs.tgz">Unix source + built packages</a>
tarball from <a href="http://racket-lang.org">Racket&rsquo;s homepage</a> because I
only wanted to compile the core of Racket. After unpacking the tarball
I was suprised that the instructions were quite short:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>From this directory (where the `configure' file is), run the following
</span><span class='line'>commands:
</span><span class='line'>
</span><span class='line'>  mkdir build
</span><span class='line'>  cd build
</span><span class='line'>  ../configure
</span><span class='line'>  make
</span><span class='line'>  make install</span></code></pre></td></tr></table></div></figure>


<p>Between <code>make</code> and <code>make install</code> I had to wait for about 40 minutes
but then everything was fine and I could even use DrRacket on the
Raspberry Pi:</p>

<p><img src="http://www.dbrunner.de/img/2015-08-27-racket-pi.png" alt="DrRacket on Raspberry Pi" /></p>

<p>Very nice and easy to get Racket running on ARM.</p>

<p>PS: Because the Raspberry Pi 2 Model B has an ARMv7 processor the
binary runs on my Jolla smart phone as well.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Racket on AWS Lambda]]></title>
    <link href="http://www.dbrunner.de/blog/2015/08/27/running-racket-on-aws-lambda/"/>
    <updated>2015-08-27T12:46:57+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/08/27/running-racket-on-aws-lambda</id>
    <content type="html"><![CDATA[<p>I started to use AWS for some projects recently. But I only use few of
their services. From time to time I look into some of there services
and wonder if they are useful for my tasks. I looked into
<a href="http://aws.amazon.com/lambda">AWS Lambda</a>, &ldquo;&hellip; a compute service
that runs your code in response to events and automatically manages
the compute resources for you, making it easy to build applications
that respond quickly to new information.&rdquo; Nowadays these &ldquo;lambda
functions&rdquo; could be written in NodeJS or Java. When I was looking for
a roadmap of the supported languages I found an interesting
<a href="http://blog.0x82.com/2014/11/24/aws-lambda-functions-in-go/">blog post</a>
by <a href="https://www.twitter.com/rubenfonseca">Ruben Fonseca</a>. He explaind
how to run Go code on AWS Lambda.</p>

<p>I tried the same with <a href="http://racket-lang.org">Racket</a> and wrote a
short Racket programm <code>test.rkt</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#lang racket/base
</span><span class='line'>
</span><span class='line'>(display (format "Hello from Racket, args: ~a~%" (current-command-line-arguments)))</span></code></pre></td></tr></table></div></figure>


<p>Then I used <code>raco</code> to create a binary <code>test</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>raco exe --orig-exe test.rkt</span></code></pre></td></tr></table></div></figure>


<p>I took the NodeJS wrapper from Ruben&rsquo;s blog post and put it in a file
<code>main.js</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var child_process = require('child_process');
</span><span class='line'>
</span><span class='line'>exports.handler = function(event, context) {
</span><span class='line'>  var proc = child_process.spawn('./test', [ JSON.stringify(event) ], { stdio: 'inherit' });
</span><span class='line'>
</span><span class='line'>  proc.on('close', function(code) {
</span><span class='line'>    if(code !== 0) {
</span><span class='line'>      return context.done(new Error("Process exited with non-zero status code"));
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    context.done(null);
</span><span class='line'>  });
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Then I put both files in a zip archive, created a new AWS Lambda
function, uploaded the zip file and invoked the function:</p>

<p><img src="http://www.dbrunner.de/img/2015-08-27-racket-aws-lambda.png" alt="Invocation of AWS Lambda function" /></p>

<p>Fine!</p>

<p>PS: Only question is: When is AWS Lambda coming to the region
<code>eu-central-1</code>, located in Frankfurt?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lisp und ich]]></title>
    <link href="http://www.dbrunner.de/blog/2015/08/27/lisp-und-ich/"/>
    <updated>2015-08-27T10:57:45+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/08/27/lisp-und-ich</id>
    <content type="html"><![CDATA[<p>Dieser Beitrag ist kurzer Hintergrund für meine bisherigen und
zukünftigen Beiträge zur Programmierung und
Software-Entiwcklung. Eigentlich ist es eher ein <em>Disclaimer</em>, denn
ich habe das gar nicht professionell gelernt. Ich habe ein
sozialwissenschaftliches Fach studiert und meine Kenntnisse aus dem
Bereich der Programmierung entstammen im Wesentlichen meiner Schul-
und Studienzeit und sehr viel autodidaktes Lernen. Insofern ist aller
Code von mir mit einer gewissen Vorsicht zu genießen; möglicherweise
ist das nicht immer die beste und schönste Variante ein Problem zu
lösen.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Der Rundfunkrat des hr übt den Diskurs]]></title>
    <link href="http://www.dbrunner.de/blog/2015/02/17/der-rundfunkrat-des-hr-ubt-den-diskurs/"/>
    <updated>2015-02-17T11:44:35+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/02/17/der-rundfunkrat-des-hr-ubt-den-diskurs</id>
    <content type="html"><![CDATA[<p>Am 15. Dezember 2014 hat der
<a href="http://www.bundesfinanzministerium.de/Web/DE/Ministerium/Geschaeftsbereich/Wissenschaftlicher_Beirat/wissenschaftlicher_beirat.html">Wissenschaftliche Beirat des Bundesministeriums des Finanzen</a>
ein Gutachten zu &ldquo;Öffentlich-rechtliche Medien - Aufgabe und
Finanzierung&rdquo;
<a href="http://www.bundesfinanzministerium.de/Content/DE/Downloads/Broschueren_Bestellservice/2014-12-15-gutachten-medien.pdf?__blob=publicationFile&amp;v=9">veröffentlicht</a>.</p>

<p>Der Inhalt wollte dem einen oder anderen Vertreter der
öffentlich-rechtlichen Medien nicht schmecken. So haben sich der Runkfunkrat
und der Verwaltungsrat des Hessischen Rundfunks &ldquo;ausführlich&rdquo; mit dem
Gutachten und einer Stellungnahme des &ldquo;hr-Justitiars&rdquo; befasst und
einen &ldquo;einstimmigen Beschluss&rdquo; hierzu
<a href="http://www.hr-online.de/website/extern/rundfunkrat/download.jsp?key=standard_document_54315171&amp;row=0&amp;rubrik=62561">gefasst</a>.</p>

<p>Ich möchte nun nicht auf sämtliche Aspekte des Gutachtens und was
dafür oder dagegen spricht eingehen. Allerdings hat mich das eine oder
andere in dem &ldquo;Beschluss&rdquo; der hr-Gremien etwas stutzig
gemacht. Es heißt dort gleich zu Beginn:</p>

<blockquote><p>&ldquo;1. Der Rundfunkrat und der Verwaltungsrat kritisieren, dass sich ein
   für Rundfunktfragen unstreitig unzuständiges Gremium ein Papier
   veröffentlicht hat, das längst überholte ökonomische Positionen
   wieder aufleben lässt und Auffassungen vertritt, die sich weder
   ökonomisch noch rechtlich halten lassen.&rdquo;</p></blockquote>

<p>In Deutschland hat sich neben der Wirtschaftstheorie und
Wirtschaftspolitik ein eigenes Gebiet der &ldquo;Finanzwissenschaft&rdquo; als
Lehre von den öffentlichen Haushalten, der Besteuerung, der
Staatsausgaben, der öffentlichen Schuld, der Staatstätigkeit
etc. herausgebildet. Einige Vertreter dieses Faches sind Mitglieder
des Wissenschaftlichen Beirats beim Bundesministerium der Finanzen.</p>

<p>Eine der ersten Fragen in der Finanzwissenschaft, die man sich
bspw. in der universitären Ausbildung stellt, ist: &ldquo;Was gehört
eigentlich zum &lsquo;Staat&rsquo;?&rdquo; Mitunter zuerst fallen einem die
Gebietskörperschaften (Bund, Länder und Gemeinden) und ihre Haushalte
ein. Daneben spielen aber auch öffentlich-rechtliche Institutionen
eine bedeutsame Rolle, die man mit dem Begriff der &ldquo;Parafisken&rdquo;
bezeichnet, sie erfüllen öffentliche Aufgaben und finanzieren sich in
der Regel über Zwangsabgaben. Hierzu zählt man beispielsweise die
Sozialversicherungen, aber auch Kammern, soweit sie öffentliche
Aufgaben wahrnehmen. Auch sie sind Gegenstand der
Finanzwissenschaft. In meinem Exemplar von Zimmermann/Henke,
Finanzwissenschaft, 7. Auflage, 1994 sind einige Ausführungen zu den
Parafisken auf den Seiten 8 bis 11 zu finden.</p>

<p>Und hierzu gehören aus diesem Blickwinkel unstreitig auch die
Institutionen des öffentlich-rechtlichen Rundfunks. Eine etwas
ausführlichere Darstellung zu den Parafisken findet sich in einem
<a href="http://www.diw.de/sixcms/detail.php?id=diw_01.c.41304.de">Gutachten des DIW Berlin von C. Katharina Spieß</a>
aus dem Jahr 2014 (das allerdings nicht den Rundfunk, sondern
familienpolitische Leistungen zum wesentlichen Inhalt hat) in den
Kapiteln 2 bis 3, für den öffentlich-rechtlichen Rundfunk im Abschnitt
3.3.</p>

<p>Wenn nun die öffentlich-rechtlichen Rundfunkanstalten Parafisken sind,
die Parafisken zum Gegenstand der Finanzwissenschaft gehören, dann
wäre es meines Erachtens sträflich, würde sich der Wissenschaftliche
Beirat beim BMF nicht damit befassen.</p>

<p>Insofern halte ich die Behauptung, der Beirat sei &ldquo;unstreitig
unzuständig&rdquo; für falsch und irreführend.</p>

<p>Außerdem beschreibt die
<a href="http://www.bundesfinanzministerium.de/Web/DE/Ministerium/Geschaeftsbereich/Wissenschaftlicher_Beirat/Satzung/satzung.html">Satzung</a>
des Beirates seine Aufgaben recht weit:</p>

<blockquote><p> &ldquo;§ 1 Aufgaben des Beirats</p>

<p>Der Beirat soll den Bundesminister der
Finanzen in voller Unabhängigkeit und ehrenamtlich in allen Fragen
der Finanzpolitik beraten.&#8221;</p></blockquote>

<p>Die hr-Gremien haben insgesamt 7 Punkte beschlossen. Obwohl sie in dem
oben zitierten ersten Punkt dem Beirat Auffassungen unterstellen, die
sich ihrer Ansicht nach ökonomisch nicht halten lassen, so finden sich
in den beschlossenen Punkten nahezu keinerlei ökonomische
Entgegenungen:</p>

<ul>
<li>In Punkt 2 werden die unterstellten Marktmechanismen in Bezug auf
den Zeitungsmarkt in Frage gestellt.</li>
<li>In Punkt 3 wird im Kern argumentiert, man hätte doch auch das
Angebot des öffentlich-rechtlichen Rundfunks beachten sollen.</li>
</ul>


<p>Demgegenüber wird bis auf Punkt 2 durchgehend mit
&ldquo;verfassungsrechtlich&rdquo; (kommt als Wort insgesamt 5mal vor) oder dem
&ldquo;Bundesverfassungsgericht&rdquo; (4 Vorkommen, 3mal als
&ldquo;Bundesverfassungsgericht&rdquo; und einmal als &ldquo;höchste deutsche Gericht&rdquo;)
argumentiert.</p>

<p>Zum Schutz der hr-Gremien muss man allerdings noch erwähnen, dass der
Beirat in seinem Gutachten neben eher finanzpolitischen Überlegungen
auch einen Abschnitt über verfassungsrechtliche Perspektiven verfasst
hatte.</p>

<p>Es bleibt aber dabei, dass die hr-Gremien sich bis auf ihren Punkt 2
mit den ökonomischen Fragestellungen aus dem Gutachten eigentlich
nicht recht auseinandersetzen wollen. Dabei wäre das dringend
nötig. Die Auseinandersetzungen um Druckerzeugnisse vs. Rundfunk,
&ldquo;Tagesschau-App&rdquo;, Depublizierung, neuartige Rundfunkgeräte,
etc. machen meines Erachtens deutlich, dass hier ein Austausch von
Argumenten dringend nötig wäre. Hierzu haben die hr-Gremien allerdings
entweder nicht die Kraft oder nicht das Verlangen. Statt dessen werfen
sie dem Wissenschaftlichen Beirat eine Entgleisung vor, wenn sie
schreiben:</p>

<blockquote><p>&ldquo;Das Ignorieren der mittlerweile 50jährigen kontinuierlichen
Rechtsprechung des Bundesverfassungsgerichts zur dualen
Rundfunkordnung &hellip; ist &hellip; nicht nur eine indiskutable Ohrfeige an
das höchste deutsche Gericht, &hellip;&rdquo;</p></blockquote>

<p>An diesem Zitat zeigt sich für mich auch, dass ein Austausch von
Argumenten wohl gar nicht gewollt ist. Die Positionen des Beirats werden
mit kernigen Formulierungen belegt: &ldquo;Auffassungen vertritt, die sich weder
ökonomisch noch rechtlich halten lassen&rdquo;, &ldquo;indiskutable Ohrfeige an
das höchste deutsche Gericht&rdquo;, &ldquo;mit der Entwicklung der
Rundfunkordnung &hellip; in keiner Weise ernsthaft auseinandergesetzt&rdquo;,
&ldquo;verkennt in erschreckender Weise&rdquo;, &ldquo;völlig unverständlich&rdquo;, &ldquo;völlig
abwegig&rdquo;, &ldquo;hat sich keinerlei Mühe gemacht&rdquo;, &ldquo;macht deutlich, wie
wenig sich der Beirat mit den rechtlichen und verfassungsrechtlichen
Fakten befasst hat&rdquo;, &ldquo;völlig untauglich&rdquo;, &ldquo;keine Alternative&rdquo; und im
letzten Satz &ldquo;[a]ngesichts der vielfachen Defizite des Papiers kann es
auch nicht den Anspruch erheben, wissenschaftlich zu sein.&rdquo;</p>

<p>Im Ergebnis: In meinen Augen zeigen die hr-Gremien einen oft zu
beobachtender Reflex: Die Gegenseite wird zuerst einmal für nicht
zuständig erklärt, anschließend ihre Argumente ignoriert und auf einer
anderen Ebene gegen sie argumentiert. Dies passiert schlussendlich
noch in einem Duktus, der auch keine Antwort mehr erwartet.</p>

<p>Insgesamt doch eher unbefriedigend und nicht von der Fähigkeit zum
Diskurs gekennzeichnet. Vielleicht hätte man nicht nur den
hr-Justitiar, sondern auch den hr-Ökonomen befragen sollen.</p>

<p>PS: Nach wie vor macht mich etwas stutzig, wie
<a href="http://www.hr-online.de/website/extern/rundfunkrat/index.jsp?rubrik=45412&amp;key=standard_document_3728464">Rolf-Dieter Postlep</a>
das hat unterschreiben können.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lexmarks Druckerpatronen-Lizenz]]></title>
    <link href="http://www.dbrunner.de/blog/2015/02/17/lexmarks-druckerpatronen-lizenz/"/>
    <updated>2015-02-17T11:28:02+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/02/17/lexmarks-druckerpatronen-lizenz</id>
    <content type="html"><![CDATA[<p>Heute benötigte ich eine Ersatzpatrone für meinen
Lexmark-Drucker. An der Aufreißlasche prangen Ausrufezeichen und der
Hinweis: &ldquo;Attention: Updated License Terms&rdquo;.</p>

<p>Lizenzbedingungen? Für eine Druckerpatrone? Also mal ein Blick aufs
Kleingedruckte:</p>

<blockquote><p>Bitte vor dem Öffnnen lesen. Durch das Öffnen der Verpackung oder
die Verwendung der mitgelieferten patentierten Kassette erklären Sie
sich mit der folgenden Lizenz-Vereinbarung einverstanden. Diese
patentierte Tonerkassette wird zu einem Sonderpreis verkauft und
unterliegt der Patenteinschränkung, dass sie nur einmal verwendet
wird. Nach ihrer erstmaligen Verwendung verpflichten Sie sich, sie
zur Wiederaufbereitung und/oder zum Recylcing nur an Lexmark
zurückzugeben. Die Tonerkassette funktioniert nach der Abgabe einer
bestimmten Tonermenge nicht mehr. Wenn sie ersetzt werden muss, kann
sie noch Resttoner enthalten. Die Kassette ist zusätzlich so
konzipiert, dass die Informationen zur Kassettenkompatibilität im
Druckerspeicher automatisch aktualisiert werden. Auf diese Weise
kann die Verwendung gefälschter Kassetten und/oder bestimmer
Drittprodukte eingeschränkt werden. Durch die Installation der
beiliegenden Kassette gestatten Sie Lexmark, diese Änderungen
vorzunehmen. Wenn Sie mit den vorgenannten Bedingungen nicht
einverstanden sind, geben Sie die ungeöffnete Verpackung an Ihren
Händler zurück. Nicht im Rahmen dieser Bestimmungen verkaufte
Ersatztonerkassetten sind unter www.lexmark.com erhältlich.</p></blockquote>

<p>Irgendwie ja auch ein bisschen putzig, wie um einen Alltagsgegenstand
wie Toner so ein Bohei gemacht wird. Zwei Gedanken kommen mir da in
den Sinn: 1. Es verfestigt sich mein Eindruck, dass das Patentsystem
recht nahe an kaputt ist. 2. Unternehmen, die so etwas machen, sollten
weniger Geld für Juristen, Patentanwälte ausgeben und das Geld eher in
coole Produkte investieren.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spielt ALBA Berlin nun Handball oder Basketball? Der Sportschau ist es egal]]></title>
    <link href="http://www.dbrunner.de/blog/2015/01/30/sportschau-handball-oder-basketball/"/>
    <updated>2015-01-30T11:32:49+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/01/30/sportschau-handball-oder-basketball</id>
    <content type="html"><![CDATA[<p>Mit Stand vom 14. Januar 2015 erschien auf der Homepage der
<a href="http://www.sportschau">Sportschau</a> ein
<a href="http://www.sportschau.de/weitere/basketball/eurocup144.html">Artikel</a>
über die Bamberger Basketballer. Darin findet sich ein Satz über die
ALBA Berlin (ebenfalls eine Basketball-Mannschaft):</p>

<blockquote><p>ALBA Berlin dominiert aktuell in der Liga und ist das einzig
verbliebene deutsche Team in der Euro League, der Champions League
des Handballs.</p></blockquote>

<p>Ah! ALBA spielt auch noch Handball? Oder doch nicht? Jedenfalls sollte
da wohl &ldquo;in der Euro League, der Champions League des Basketballs&rdquo;
stehen, denn die
<a href="https://de.wikipedia.org/wiki/ULEB_Euroleague">Euro League</a> ist ein
europäischer Wettbewerb von Basketball-Vereinsmannschaften.</p>

<p>Denke ich mir also: &ldquo;Ach, wie lustig, das kann ja mal passieren&rdquo; und
twittere die Sportschau am
<a href="https://twitter.com/Krrrcks/status/555840754858610688">15. Januar 2015</a>
und
<a href="ttps://twitter.com/Krrrcks/status/556751066587734016">18. Januar 2015</a>
mit einem Hinweis auf den Tippfehler an. Beim heutigen Durchblättern
der Sportschau-Seite stieß ich erneut auf den Text und der Fehler ist
heute, also am 30. Januar 2015, nach wie vor dort zu sehen:</p>

<p><img src="http://www.dbrunner.de/img/2015-01-30-sportschau.png" alt="Homepage Sportschau" /></p>

<p>Ich meine, man kann sich ja mal vertun, aber dann auf einen
Leserhinweis so gar nichts zu unternehmen, ist der Sportschau eigentlich
nicht würdig.</p>

<p><em>Update:</em> Ich twitterte die Sportschau mit meinem Blog-Post an und
 unmittelbar danach wurde der Fehler
 <a href="https://twitter.com/sportschau/status/561138889776447489">korrigiert.</a>
 Sehr prompte Reaktion.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[3. Docker-Meetup in Frankfurt]]></title>
    <link href="http://www.dbrunner.de/blog/2015/01/18/3-docker-meetup-in-frankfurt/"/>
    <updated>2015-01-18T20:05:35+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/01/18/3-docker-meetup-in-frankfurt</id>
    <content type="html"><![CDATA[<p>Am 13. Januar 2015  fand in Frankfurt das bereits <a href="http://www.meetup.com/Docker-Frankfurt/events/219160756/">dritte Docker-Meetup</a>
statt, hier einige Notizen von mir dazu.</p>

<h2>Neues zum Them Orchestrierung</h2>

<p><a href="https://www.twitter.com/PRossbach">Peter Rossbach</a> hat in einem
munteren Vortrag einige Neuerungen aus dem &ldquo;Docker Universum&rdquo; zum
Thema Orchestrierung vorgestellt. Unter anderem
<a href="https://github.com/docker/machine">Docker Machine</a>,
<a href="https://github.com/docker/swarm">Docker Swarm</a>, Docker Compose
(ehemals <a href="http://fig.sh">fig.sh</a>, das wohl aufgrund von
Aussprachemehrdeutigkeiten umbenannt wurde) etc. Ein sehr
interessanter Überblick, insbesondere da Peter auch die ganzen Sachen
immer mal angefasst und ausprobiert hat. Im Kern scheint es mir jedoch
so zu sein, als wäre die Frage nach &ldquo;Was nimmt man am besten, um
Docker auf einer oder mehreren Maschinen im Produktivbetrieb zu
nutzen?&rdquo; noch recht in Bewegung. Für mich kristallisiert sich für
meine Anwendungsfälle da bisher noch keine überzeugende Lösung
heraus. Was ich jedoch einmal testen werde ist das fig.sh bzw. Docker
Compose, da man damit eigentlich sehr schön in einem YAML-Dokument
mehrere Container und ihre Abhängigkeiten darstellen kann.</p>

<h2>Docker Linking</h2>

<p><a href="http://linsenraum.de">Erkan Yanar</a> hat in einem Einsteigervortrag die
Grundlagen von Links zwischen Containern vorgestellt. Hier scheint die
Entwicklung auch noch in Bewegung zu sein, insbesondere Links über
mehrere Hosts hinweg scheinen doch noch nicht so ganz einfach
handzuhaben zu sein (vorgestellt wurden
<a href="https://github.com/SvenDowideit/dockerfiles/blob/master/ambassador/Dockerfile">Ambassador-Ansätze</a>
mit <a href="http://www.dest-unreach.org/socat/">socat</a> und anderes).</p>

<p>Besonders erhellend fand ich den Hinweis, dass ab Docker Version 1.3
nun bei Links zwischen den Containern die <code>/etc/hosts</code> auch nach
Neustarts von gelinkten Containern immer deren richtige IP-Adresse
erhält, wohingegen die Umgebungsvariablen nur die
Ursprungs-IP-Adressen enthalten (also ein klares &ldquo;Verlasst Euch nicht
auf die Umgebungsvariablen!&rdquo;).</p>

<h2>Netzwerken mit Docker</h2>

<p><a href="https:/www.twitter.com/aschmidt75">Andreas Schmidt</a> stellte eine
ganze Reihe von Varianten vor, mit denen man die Container im Netzwerk
auf unterschiedliche Arten und Weisen verknoten kann. Soweit ganz
interessant, aber nicht meine &ldquo;Liga&rdquo;, wo ich mich gut auskenne.</p>

<h2>Fazit</h2>

<p>Bei Docker in Bezug auf Orchestrierung und Container-Linken gibt es
recht viel Bewegung und für mich kristallisieren sich die
überzeugenden Konzepte noch nicht so richtig heraus, um damit in eine
Produktivumgebung zu gehen. Im Bereich der Entwicklung und der Tests
nutze ich die Container von Docker schon recht gerne, bei
Produktiv-Umgebungen schreckt mich die Vielzahl an Werkzeugen und zum
Teil auch die Komplexität doch noch etwas.</p>

<p>Jedenfalls wieder ein gutes Meetup mit Ideen und Anregungen. Ich finde
das schon sehr außergewöhnlich (besonders wenn man es mit anderen
Branchen vergleicht), dass sich Leute zum Austauschen über Technologie
treffen, Vorträge vorbereiten etc.</p>

<h2>Links zu den Folien</h2>

<ul>
<li><a href="https://speakerdeck.com/rossbachp/docker-meetup-frankfurt-2015-docker-orchestration">Peter Rossbach, Docker Orchestation</a></li>
<li><a href="https://speakerdeck.com/aschmidt75/docker-networking">Andreas Schmidt, Docker Networking</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Eindrücke vom 31C3]]></title>
    <link href="http://www.dbrunner.de/blog/2015/01/05/eindrucke-vom-31c3/"/>
    <updated>2015-01-05T17:03:00+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/01/05/eindrucke-vom-31c3</id>
    <content type="html"><![CDATA[<p>Dieses Jahr habe ich mich einmal aufgerafft und bin das erste mal zum
Congress des CCC nach Hamburg gefahren. Im Folgenden ein paar
Eindrücke:</p>

<h2>Drumherum</h2>

<p>Ich fand den <a href="http://events.ccc.de/congress/2014/wiki/Main_Page">31C3</a> ein tolles Ereignis: Sehr professionell und
umsichtig organisiert. Überall, wo ich hinkam, war man nett und
freundlich, insgesamt eine sehr willkommende Atmosphäre. Nur der
Termin, der ist ja doch etwas sperrig.</p>

<h2>Vorträge</h2>

<p>Ich konnte einige Vorträge anhören, von denen haben mir die folgenden
besonders gut gefallen (ich habe mal die Links zu den Videos und die Links zu den Einträgen im Fahrplan aufgeführt, oft gibt es im Fahrplan auch noch zugehöriges Material und weitere Hinweise):</p>

<ul>
<li><p><a href="http://media.ccc.de/browse/congress/2014/31c3_-_6264_-_de_-_saal_1_-_201412271245_-_wir_beteiligen_uns_aktiv_an_den_diskussionen_-_martin_haase_maha.html">&ldquo;Wir beteiligen uns aktiv an den Dikussionen&rdquo;</a> <a href="http://events.ccc.de/congress/2014/Fahrplan/events/6264.html">(Link im Fahrplan)</a> von Martin Haase, der die <a href="http://www.digitale-agenda.de/">Digitale Agenda</a> der Bundesregierung aus sprachwissenschaftlicher Sicht entlarvt als das, was es ist: Heiße Luft und wenig Konkretes, schon gar nicht für den &ldquo;Bürger&rdquo;. Besonders spannend fand ich den Teil des &ldquo;PDF befreien&rdquo;, denn die Bundesregierung hat nur ein wenig konsistentes PDF bereit gestellt, dass maha erst einmal in einen Text umwandeln musste, mit dem er mit seinen Werkzeugen arbeiten konnte. In der Diskussion wurde er dazu auch noch einmal befragt und meinte, Markdown, das sei eigentlich ein ganz gutes Format.</p></li>
<li><p>Die beiden SS7-Vorträge von <a href="http://media.ccc.de/browse/congress/2014/31c3_-_6249_-_en_-_saal_1_-_201412271715_-_ss7_locate_track_manipulate_-_tobias_engel.html">Tobias Engel</a> <a href="http://events.ccc.de/congress/2014/Fahrplan/events/6249.html">(Fahrplan)</a> und <a href="http://media.ccc.de/browse/congress/2014/31c3_-_6122_-_en_-_saal_1_-_201412271830_-_mobile_self-defense_-_karsten_nohl.html">Karsten Nohl</a> <a href="http://events.ccc.de/congress/2014/Fahrplan/events/6122.html">(Fahrplan)</a>: Die Talks fand ich incl. der Live-Vorführungen sehr eindrücklich und haben mir vor Augen geführt, dass es mit der Sicherheit im Mobilfunk noch schlechter aussieht, als ich so befürchtet habe.</p></li>
<li><p><a href="http://media.ccc.de/browse/congress/2014/31c3_-_6369_-_en_-_saal_1_-_201412272145_-_ecchacks_-_djb_-_tanja_lange.html">ECCHacks</a> <a href="http://events.ccc.de/congress/2014/Fahrplan/events/6369.html">(Fahrplan)</a> von djb und Tanja Lange: Ein Bekannter empfahl mir den Vortrag und meinte, ich könnte da schon was verstehen, obwohl ich mich mit diesen Ellpitischen Kurven nicht wirklich auskenne. Der Talk war didaktisch sehr gut aufbereitet und ich habe trotz der späten Stunde ein bisschen verstanden (glaube ich), worum es da eigentlich geht.</p></li>
<li><p><a href="http://media.ccc.de/browse/congress/2014/31c3_-_6294_-_de_-_saal_1_-_201412281815_-_vor_windows_8_wird_gewarnt_-_ruedi.html">Vor Windows 8 wird gewarnt</a> <a href="http://events.ccc.de/congress/2014/Fahrplan/events/6294.html">(Fahrplan)</a> von ruedi: Ein kurzweiliger Vortrag über &ldquo;Secure Boot&rdquo; und andere Schwierigkeiten mit &ldquo;Windows 8&rdquo;.</p></li>
<li><p><a href="http://media.ccc.de/browse/congress/2014/31c3_-_6258_-_en_-_saal_1_-_201412282030_-_reconstructing_narratives_-_jacob_-_laura_poitras.html">Reconstructing naraatives</a> <a href="http://events.ccc.de/congress/2014/Fahrplan/events/6258.html">(Fahrplan)</a> von Jacob Appelbaum und Laura Poitras: Das war im voll besetzten Saal 1 ein sehr eindrücklicher Vortrag, der mit Standing Ovations endete.</p></li>
<li><p><a href="http://media.ccc.de/browse/congress/2014/31c3_-_6121_-_en_-_saal_2_-_201412291715_-_what_ever_happened_to_nuclear_weapons_-_michael_buker.html">What Ever Happened to Nuclear Weapons?</a> <a href="http://events.ccc.de/congress/2014/Fahrplan/events/6121.html">(Fahrplan)</a> von Michael Büker: Diesen Vortrag fand ich vom Aufbau und der Didaktik sehr gut vorbereitet. Als wichtige Erkenntnis habe ich für mich den <a href="http://de.wikipedia.org/wiki/Kernwaffenteststopp-Vertrag">Kernwaffenstopp-Vertrag (englisch Comprehensive Nuclear-Test-Ban Treaty)</a> mitgenommen, einen internationalen Vertrag, der sämtliche Kernwaffentests verbietet, der aber noch nicht in Kraft getreten ist; dieser Vertrag geht weiter als der Nuclear Test Ban Treaty aus den 1960er Jahren, der Kernwaffenversuche in der Atmosphäre, im Weltraum und unter Wasser verbietet. Dennoch gibt es hierzu schon eine <a href="http://www.ctbto.org/">Organisation, die Preparatory Commission for the Comprehensive Nuclear-Test-Ban Treaty Organisation</a>, der man auch auf Twitter <a href="http://twitter.com/ctbto_alerts">folgen kann</a>. Diese  &ldquo;Preparatory Commission&rdquo; bereitet das Inkraftreten vor und baut ein Überwachungssystem auf.</p></li>
<li><p><a href="http://media.ccc.de/browse/congress/2014/31c3_-_6128_-_en_-_saal_1_-_201412291830_-_thunderstrike_efi_bootkits_for_apple_macbooks_-_trammell_hudson.html">EFI bootkits for Apple MacBooks</a> <a href="http://events.ccc.de/congress/2014/Fahrplan/events/6128.html">(Fahrplan)</a> von Trammell Hudson: Ich fand das sehr spannend vorgetragen (incl. Hexdumps etc.), wie Trammel Hudson durch &ldquo;Reverse Engineering&rdquo; auf ein Sicherheitsproblem bei Apple-Produktion gestoßen ist und dieses dann in einem Proof-of-Concept auch ausnutzen konnte.</p></li>
</ul>


<p>Aufgrund des riesigen Angebots an Vorträgen, Workshops etc. muss ich
mir in den kommenden Tagen glaube ich noch den einen oder anderen
Vortrag als Video ansehen.</p>

<h2>MorgenGrauen-Stammtisch</h2>

<p>Etwas spontan und nicht so sonderlich koordiniert haben wir auch einen
<a href="http://mg.mud.de">MorgenGrauen</a>-Stammtisch ausgerufen und siehe da: Drei Spieler und zwei
Gäste fanden sich ein, so dass man bei einem Bier ein wenig plaudern
und sich austauschen konnte.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Aus zwei mach eins]]></title>
    <link href="http://www.dbrunner.de/blog/2015/01/04/aus-zwei-mach-eins/"/>
    <updated>2015-01-04T16:16:08+00:00</updated>
    <id>http://www.dbrunner.de/blog/2015/01/04/aus-zwei-mach-eins</id>
    <content type="html"><![CDATA[<p>Bisher hatte ich meine Blog-Einträge auf zwei Blogs aufgeteilt,
<a href="http://blog.krrrcks.net">eines</a> mit mehr technischen (und zum Teil
englischen Texten) und dieses hier mit deutschen Texten. Ich denke,
ich werde das auf dieses eine Blog hier konzentrieren. Das reduziert
doch etwas den Verwaltungsaufwand. Ich habe die Texte vom nun etwas
still gelegten Blog hier herüber kopiert.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ich habe mal Octopress aktualisert]]></title>
    <link href="http://www.dbrunner.de/blog/2014/12/21/ich-habe-mal-octopress-aktualisert/"/>
    <updated>2014-12-21T11:58:16+00:00</updated>
    <id>http://www.dbrunner.de/blog/2014/12/21/ich-habe-mal-octopress-aktualisert</id>
    <content type="html"><![CDATA[<p>Neulich fiel mir auf, dass die Suchfunktion, die auf Google basiert,
nicht nur auf der eigenen Homepage, sondern im &ldquo;gesamten&rdquo; Suchindex
von Google gesucht hat. Eben flatterte dieser
<a href="https://twitter.com/octopress/status/546528904115404800">Tweet</a> mit
<a href="https://github.com/imathis/octopress/commit/514ed5eb9f6bb91a6f3288febf3c2ba892a9b693">Link</a>
zu einem Fix an mir vorbei und da hat sich wohl die API geändert.</p>

<p><img src="http://www.dbrunner.de/img/2014-12-21-octopress.png" alt="Octopress-Tweet" /></p>

<p>Also habe ich mich einmal an den
<a href="http://octopress.org/docs/updating/">Hinweisen</a> zum Aktualisieren von
Octopress orientiert. Hierzu werden die folgenden Schritte vorgeschlagen:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git pull octopress master      # Get the latest Octopress
</span><span class='line'>bundle install                 # Keep gems updated
</span><span class='line'>rake update_source             # update the template's source
</span><span class='line'>rake update_style              # update the template's style</span></code></pre></td></tr></table></div></figure>


<p>Soweit lief das auch. Es tauchten anschließend zwei Probleme auf:</p>

<ol>
<li><p>Das <code>execjs</code> jammerte, dass ihm irgendwie eine
JavaScript-Bibliothek fehlte. Nach kurzer Recherche gab es die
Empfehlung, die Zeile <code>gem 'therubyracer'</code> in das <code>Gemfile</code>
aufzunehmen. Okay, das funktionierte schon einmal.</p></li>
<li><p>Bei den Feeds für die Kategorien jammerte das Octopress ein
ungültiges Layout an. In <code>category_feed.xml</code> stand in der Tat
<code>layout: nil</code>. Ich habe das auf <code>layout: page</code> geändert. Nun tat
auch dies.</p></li>
</ol>


<p>Mit dem Update funktioniert nun auch das Such-Formular wieder.</p>

<p><strong>Ergänzung:</strong> Ich lasse das Jekyll und Octopress in einem
   <a href="http://www.dbrunner.de/blog/2014/06/10/relaunch-fast-fertig/">Docker-Container</a> laufen und irgendwie bekam ich bei <code>rake preview</code>
   meine Seite nicht mehr zu sehen. Da ich mich mit Ruby und den
   Komponenten nicht so gut auskenne, weiß ich nicht, ob mein &ldquo;Fix&rdquo; so
   gut ist: Ich habe im <code>Rakefile</code> den <code>rackup</code>-Aufruf um den
   Parameter <code>-o 0.0.0.0</code> ergänzt. Dann bekam ich vom Host wieder eine
   Verbindung zum Webserver im Octopress-Container.</p>

<p><strong>Update vom 04. Januar 2014:</strong> Nach einem kurzen
  <a href="https://twitter.com/Krrrcks/status/546667456128114688">Gespräch</a>
  via Twitter habe ich mich entschieden, den eingebauten Server doch
  wieder so zu belassen, wie es im <code>Rakefile</code> ursprünglich
  stand. Statt dessen habe ich einfach in dem Docker-Container einen
  <code>nginx</code> hinzuinstalliert, der dann mein <code>public</code>-Verzeichnis an den
  Port 4000 ausliefert. So funktioniert es wieder prima: Ich starte
  den <code>nginx</code> und lasse dann das <code>rake watch</code> laufen, um die Seiten
  immer wieder neu zu erzeugen, wenn sich Dinge geändert haben.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA["Neues" Arbeitspapier]]></title>
    <link href="http://www.dbrunner.de/blog/2014/12/18/neues-arbeitspapier/"/>
    <updated>2014-12-18T18:24:27+00:00</updated>
    <id>http://www.dbrunner.de/blog/2014/12/18/neues-arbeitspapier</id>
    <content type="html"><![CDATA[<p>Zusammen mit
<a href="http://www.bk.tudelft.nl/en/about-faculty/departments/otb/about-otb/staff/alle-medewerkers/haffner-marietta/">Marietta Haffner</a>
hatte ich für die
<a href="http://www.ccnorway.no/enhr2012/">ENHR-Konferenz im Jahr 2012 in Lillehammer</a>
ein Konferenzpapier vorbereitet, das Marietta dort vorgestellt
hatte. Wir haben nun in der Arbeitspapier-Reihe des OTB an der TU
Delft eine überarbeitete Fassung <a href="http://www.bk.tudelft.nl/fileadmin/Faculteit/BK/Over_de_faculteit/Afdelingen/OTB/publicaties/Working_papers/OTB_Working_papers_2014-07_German_cooperatives.pdf">eingestellt</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Aus der Entscheidung des BVerfG zur Erbschaftsteuer]]></title>
    <link href="http://www.dbrunner.de/blog/2014/12/18/aus-der-entscheidung-des-bverfg-zur-erbschaftsteuer/"/>
    <updated>2014-12-18T07:41:37+00:00</updated>
    <id>http://www.dbrunner.de/blog/2014/12/18/aus-der-entscheidung-des-bverfg-zur-erbschaftsteuer</id>
    <content type="html"><![CDATA[<p>Das Bundesverfassungsgerichts hat in seiner <a href="http://www.bundesverfassungsgericht.de/SharedDocs/Entscheidungen/DE/2014/12/ls20141217_1bvl002112.html">Entscheidung</a>
vom 17. Dezember 2014 (1 BvL 21/12) Regelungen zur Erbschaftsteuer als
verfassungwidrig moniert. Ich finde in der Entscheidung eigentlich den
fünften Leitsatz mit am interessantesten:</p>

<blockquote><p>&ldquo;Ein Steuergesetz ist verfassungswidrig, wenn es Gestaltungen zulässt,
mit denen Steuerentlastungen erzielt werden können, die es nicht
bezweckt und die gleichheitsrechtlich nicht zu rechtfertigen sind.&rdquo;</p></blockquote>

<p>Nun bin ich kein Steuer-, Verfassungsrechtsspezialist (genaugenommen
gar kein Rechtsspezialist) und weiß nicht, ob dieser Satz nicht
ohnehin schon gilt oder Verfassungs- und Steurrechtsrealität ist. Ich
habe es allerdings in dieser Klarheit noch nicht gelesen. Und wenn ich
mir so das eine oder andere Steuergesetz und seine Wirkungen ansehe,
dann kommt da unter diesem Leitsatz vielleicht in den kommenden Jahren
einiges auf die Gerichte zu.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Migration alter Blog-Einträge]]></title>
    <link href="http://www.dbrunner.de/blog/2014/12/17/migration/"/>
    <updated>2014-12-17T18:11:00+00:00</updated>
    <id>http://www.dbrunner.de/blog/2014/12/17/migration</id>
    <content type="html"><![CDATA[<p>Es kamen lange Winterabende und ich habe die alten Blog-Einträge von
der mittels Org-Mode erstellten Seite hier nach Octopress
migriert. Im Archiv sind diese zu finden und die Links etc. sollten
nun auch passend mit migriert worden sein.</p>

<p>Bei der Konvertierung habe ich ein Programm kennengelernt, das mir
sehr geholfen hat: <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>. Das
ist ein Konverter, der Text- und Markupdateien hin und her
konvertieren kann. Dabei beherrscht er eine ganze Reihe von
Formaten. Insbesondere kann er das Org-Mode-Format (das ich für viele
Dinge verwende) ebenso wie LaTeX und Markdown (daneben aber auch noch
eine Legion von Formaten).</p>

<p>Da mein Hauptdateiformat ohnehin &ldquo;Textdateien&rdquo; sind (&ldquo;Never trust a
file that isn&rsquo;t ASCII&rdquo;) ist das sehr praktisch, um zwischen
verschiedenen Markup-Formaten hin und her zu springen.</p>

<p>Für die Migration meine Blog-Einträge lief dies wie folgt: Ich hatte
die in der alten Homepage mit Org-Mode und Org-Jekyll erstellt. Dabei
hatte das Org-Jekyll-Modul entsprechende HTML-Dateien mit einem
YAML-Header erzeugt. Mittels Pandoc konnte ich die nun wieder nach
Markdown konvertieren, habe die YAML-Header ein wenig angepasst,
Kategorien aktualisiert und noch mal das Markup zwecks Zeilenumbrüche,
Fußnoten und Links geprüft und leicht angepasst. Insgesamt sehr
schnell bewältigbar.</p>

<p>Dieser ganze Zoo rund um Markdown und andere ähnliche Formate ist zwar
manchmal ein bisschen unübersichtlich, aber mir gefällt das sehr gut,
dass man mit wenig Markup schön Textdateien strukturieren kann und
dann mit Pandoc auch gut hin und her konvertieren kann.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wie das Haus von Bundesministerin Nahles mauert]]></title>
    <link href="http://www.dbrunner.de/blog/2014/11/21/wie-das-haus-von-bundesministerin-nahles-mauert/"/>
    <updated>2014-11-21T10:39:08+00:00</updated>
    <id>http://www.dbrunner.de/blog/2014/11/21/wie-das-haus-von-bundesministerin-nahles-mauert</id>
    <content type="html"><![CDATA[<p>Ein Gesetzesvorhaben der großen Koalition ist eine Regelung zur
Tarifeinheit. Passend in die Tarifauseinandersetzungen bei der
Deutschen Bahn zeigte Bundesministerin Nahles Entschlossenheit und
Tatkraft:</p>

<p>Am 28. Oktober 2014
<a href="http://www.bmas.de/DE/Themen/Arbeitsrecht/Meldungen/gesetzentwurf-tarifeinheit.html;jsessionid=FB47DA2A8F96F14200ECE524ACBD96BC">informierte sie</a>
die Presse über den Gesetzentwurf ihres Hauses zur
Tarifeinheit. Dieser solle in die Ressortabstimmung gehen und
am 3. Dezember im Kabinett verabschieden werden. Den Abschluss des
parlamentarischen Verfahrens erwartete Frau Nahles für den Sommer
2015.</p>

<p>Über die Vorstellung und den Inhalt des Entwurfs berichtete
ausführlich in zwei Berichten die FAZ
(<a href="http://www.faz.net/-i2k-7vmd0">hier</a> und
<a href="http://www.faz.net/-gqg-7vng9">hier</a>). Den Berichten nach fand ein
Gespräch der FAZ mit der Bundesministerin statt. Der FAZ lag der
Referentenentwurf vor. Den Berichten ebenfalls zu entnehmen war, dass
sozialpolitische und arbeitsrechtliche Interessenverbände wie die
Bundesvereinigung der Arbeitgeber und auch der Deutsche
Gewerkschaftsbund über den Entwurf verfügten, diese genannten sogar an
&ldquo;der Ausarbeitung des Gesetzes beteiligt&rdquo; waren (Zitat aus dem
FAZ-Bericht).</p>

<p>Ich bin ja skeptisch, wie man das, was die Koalition sich da
vorgenommen hat, regeln kann. Also dachte ich mir: Wenn die Zeitung
davon weiß, die Interessenverbände mitgearbeitet haben und auch denen
der Entwurf vorlag und dann die Bundesministerin auch noch hierüber
Pressegespräche führt und den Entwurf öffentlich &ldquo;vorstellt&rdquo;, dann
kann ich da ja auch einmal einen Blick hineinwerfen. Ich habe, da
ich den Entwurf nicht auf der Homepage entdeckte, diesen beim
Bundesministerium für Arbeit und Soziales angefordert.</p>

<p>So einfach geht es aber nicht: Es antwortete mir am <em>29. Oktober 2014</em>
das &ldquo;Kommunikationscenter&rdquo; des Bundesministeriums für Arbeit und
Soziales wie folgt:</p>

<blockquote><p>Sollten Sie (&hellip;) den Gesetzentwurf zur Tarifeinheit meinen, so
müssen wir Ihnen mitteilen, dass dieser noch nicht veröffentlicht
wurde. Er geht in Kürze in die Ressortabstimmung und kann erst
danach veröffentlicht werden.</p></blockquote>

<p>Ich wies in einer Antwort darauf hin, dass das ja nicht so ganz sein
könnte, dass die Tarifparteien, interessierte Verbände und die Presse
den Entwurf vorliegen hätten, der gemeine Bürger sich aber mit der
Presseerklärung der Bundesministerin (ein MP3-File mit 2:36 Minuten
Länge) bescheiden müsse. Am <em>30. Oktober 2014</em> teilte das
Kommunikationscenter mit, dass sie die Anfrage an die Fachabteilung
weitergeben wollten, dafür aber noch meine Postanschrift
benötigten. Diese habe ich umgehend übermittelt. Es passierte erst
einmal nichts. Am <em>4. November 2014</em> wollte ich dann den Stand wissen
(es kann ja nicht so schwer sein, ein PDF per E-Mail zu versenden) und
außerdem, welche Fachabteilung dafür zuständig ist (dann kann man das
ja schnell per Telefon oder E-Mail klären). Nichts
passierte. Am <em>7. November 2014</em> habe ich dann noch mal erklärt, dass
ich den Entwurf wirklich möchte und die Anfrage formal auf das
Informationsfreiheitsgesetz gestützt. Nun wachte das
Kommunikationscenter am <em>10. November 2014</em> auf und teilte mit, dass sie
die Anfrage an die zuständige Fachabteilung weitergeleitet
hätten. Welche das ist, mochte man mir aber immer noch nicht
mitteilen.</p>

<p>Das war es dann wieder, das Bundesministerium verfiel erneut in
Stille. Seit dem sind zwei Wochen vergangen.</p>

<p>Da mich der Entwurf wirklich interessierte, habe ich einmal einen
Interessenverband angeschrieben und nach ca. einer Stunde erhielt ich den Entwurf.</p>

<p>Ich stelle mir da ja schon einige Fragen: Was ist eigentlich so
schwer daran, einen Referentenentwurf, den man ohnehin breit hat
zirkulieren lassen, einem Bürger zur Verfügung zu stellen? Warum
antwortet die Fachabteilung über einen Zeitraum vom 4. November 2014
bis heute einfach gar nicht?</p>

<p>Mir drängt sich jedenfalls der Eindruck auf, dass das Ministerium von
Frau Nahles die Diskussion gestalten möchte. Dabei soll wohl die
Bundesministerin als zupackende und problemlösende Politikerin
dargestellt werden. Hierzu werden Verbände und Medien exklusiv mit
Details versorgt.  Der gemeine Bürger soll sich davon beeindrucken
lassen und bloß nicht nach Details fragen; die erfährt er dann schon,
wenn die Politik und Ministerialverwaltung es für geboten halten.</p>

<p>P.S.: Ich weiß, dass Referentenentwürfe normalerweise nicht auf der
Homepage erscheinen und Gesetzesvorhaben vorab mit Verbänden
diskutiert werden. Ich halte das auch grundsätzlich für sinnvoll. Ich
habe aber bisher noch nicht ein solches &ldquo;Mauern&rdquo; erlebt. Bisher
erhielt ich dort, wo ich angefragt hatte, immer ohne Schwierigkeiten
Auskunft.</p>

<p><strong>Update (01. Dezember 2014):</strong> Zwischenzeitlich habe ich den
  Referentenentwurf für jedermann zum Download bei der taz gefunden:
  <a href="http://taz.de/static/pdf/ReferentenentwurfTarifeinheit.pdf">PDF</a></p>

<p><strong>Update (13. Dezember 2014):</strong> Am 11. Dezember 2014 hat das
  Bundeskabinett den Gesetzentwurf zur Tarifeinheit
  <a href="http://www.bundesregierung.de/Content/DE/Kabinettssitzung/2014/12/2014-12-11-kabinett.html?nn=434518">beschlossen</a>,
  er findet sich nun
  <a href="http://www.bmas.de/DE/Service/Presse/Pressemitteilungen/Tarifeinheit-staerkt-sozialpartnerschaft.html?nn=31846">auf der</a>
  Homepage des Bundesministeriums für
  Arbeit und Soziales:
  <a href="http://www.bmas.de/SharedDocs/Downloads/DE/Thema-Arbeitsrecht/entwurf-gesetz-tarifeinheit.pdf?__blob=publicationFile">PDF</a>
  Wenig erstaunlicherweise habe ich auf meine Anfrage vom 29. Oktober
  2014 außer den oben dargestellten Nachrichten des
  &ldquo;Kommunikationscenters&rdquo; nichts gehört.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Warten auf den technischen Fortschritt als politisches Prinzip?]]></title>
    <link href="http://www.dbrunner.de/blog/2014/11/13/warten-auf-den-technischen-fortschritt-als-politisches-prinzip/"/>
    <updated>2014-11-13T09:53:28+00:00</updated>
    <id>http://www.dbrunner.de/blog/2014/11/13/warten-auf-den-technischen-fortschritt-als-politisches-prinzip</id>
    <content type="html"><![CDATA[<p>Am 12. November 2014 sendete der
<a href="http://www.deutschlandfunk.de">Deutschlandfunk</a> in der Reihe
&ldquo;Hintergrund&rdquo; einen
<a href="http://www.deutschlandfunk.de/energetische-sanierung-widerstand-gegen-das-daemmen-waechst.724.de.html?dram:article_id=303005">Bericht</a>
über die energetische Sanierung mit Wärmedämmverbundsystemen.</p>

<p>Zu der Diskussion wollte sich die zuständige Bundesministerin Barbara
Hendricks nicht äußern, das Thema sei nur ein Randthema und sie habe
andere Prioritäten. Sehr interessant waren dafür dann die Meinungen des
sie vertretenden Staatssekretärs <a href="http://www.bmub.bund.de/bmub/leitung-des-hauses/lebenslauf-von-herrn-gunther-adler/">Gunther Adler</a>.</p>

<p>In diesem Blogpost will ich auf einen Aspekt des Berichts eingehen. Es
sei wohl so, dass die Entsorgung von Wärmedämmverbundsystemen (also im
Wesentlichen auf verschiedene Arten behandeltes Polystyrol) schwierig
ist. So wird gesagt:</p>

<blockquote><p>&ldquo;Das große Problem ist gerade bei der Polystyrol-Dämmung, dass die
Lebenserwartung der Dämmplatten relativ gering ist. Man geht davon
aus, dass sie 20 oder 30 Jahre an der Fassade bleiben können, dann
ersetzt werden müssen.&rdquo;</p></blockquote>

<p>Über die Entsorgung wurde gesagt:</p>

<blockquote><p>&ldquo;Das Dämmmaterial Polystyrol darf nicht auf die Mülldeponie, unter
anderem wegen der erwähnten Flammschutzmittel. Das Material soll in
Müllverbrennungsanlagen vernichtet werden, &hellip;&rdquo;</p></blockquote>

<p>Auf diese Thematik angesprochen, entgegnete der Staatssekretär unter
anderem:</p>

<blockquote><p>&ldquo;&hellip; da gibt es berechtigte Fragen, da gibt es auch nichts unter den
Tisch zu kehren. Da gibt es im Moment noch Forschungsbedarf.&rdquo;</p></blockquote>

<p>Die DLF-Redakteure wiesen hier jedoch darauf hin, dass das Material
seit Langem eingesetzt würde. In den vergangenen 35 Jahren seien 900
Millionen Quadratmeter Wärmedämmverbundsysteme verbaut worden, davon
80 Prozent mit Polystyrol. Hierauf der Staatssekretär:</p>

<blockquote><p>&ldquo;Wenn wir da schon heute die Forschung anlaufen lassen und fragen,
was passiert eigentlich in 30 oder 40 oder 50 Jahren, wenn das dann
mal in die Müllverbrennungsanlage geht, da sollten wir doch offen
für technischen Fortschritt in Deutschland sein, dass es unserer
Industrie und unserer Forschung gelingen wird, beispielsweise
Filteranlagen zu entwickeln, die wirklich garantieren, dass hundert
Prozent aller Schadstoffe abgefangen werden.&rdquo;</p></blockquote>

<p>Ich fasse zusammen: Das Bundesministerium für Umwelt, Naturschutz, Bau
und Reaktorsicherheit unterstützt offen das Inverkehrbringen von
Stoffen, deren Entsorgung zumindest &ldquo;offene Fragen&rdquo; und
&ldquo;Forschungsbedarf&rdquo; bringt und das Ministerium findet, der technische
Fortschritt wird es schon richten. Kurzum, es gilt das Prinzip
&ldquo;Hoffnung auf die Zukunft&rdquo; und die Last, sich die Entsorgung zu
überlegen, bürdet man den nachfolgenden Generationen auf.</p>

<p>Wo sind in diesem Zusammenhang eigentlich die ganzen Papiere und
politischen Ansätze zu Nachhaltigkeit, Verantwortung, Schutz
zukünftiger Generationen geblieben? Im Übrigen widerspricht die
Haltung auch dem vom Bauministerium selbst herausgegebenen
<a href="http://www.nachhaltigesbauen.de/leitfaeden-und-arbeitshilfen-veroeffentlichungen/leitfaden-nachhaltiges-bauen-2013.html">Leitfaden Nachhaltiges Bauen 2013</a>,
in dem besonderer Wert auf den Lebenszyklus von Gebäuden auch unter
dem Gesichtspunkt der Entsorgung von Materialen gelegt wird.</p>

<p>Wenn schon das Thema &ldquo;Wärmedämmverbundsysteme&rdquo; nicht die Priorität der
Bundesministerin hat, so muss sie sich fragen lassen, ob gerade im
Umwelt- und Bauressort das Prinzip &ldquo;Hoffnung auf die Zukunft&rdquo;
Leitlinie ihrer Politik ist.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Aktenführung beim Beitragsservice]]></title>
    <link href="http://www.dbrunner.de/blog/2014/10/05/aktenfuhrung-beim-beitragsservice/"/>
    <updated>2014-10-05T10:45:13+00:00</updated>
    <id>http://www.dbrunner.de/blog/2014/10/05/aktenfuhrung-beim-beitragsservice</id>
    <content type="html"><![CDATA[<p>Im Jahr 2012 habe ich mich rechtmäßig (was allerdings auch etwas
Überzeugungsarbeit kostete) von der damaligen GEZ abgemeldet und die
neue Beitragsnummer mitgeteilt, über die meine damalige Rundfunkgebühr
verbucht werden sollte. Der GEZ war meine alte Beitragsnummer, die
neue Beitragsnummer sowie meine Anschrift bekannt. Soweit so gut,
besondere Veränderungen sind in der Zwischenzeit nicht eingetreten.</p>

<p>Am <strong>17. April 2014</strong> erreicht mich ein Schreiben des
Beitragsservices. Sie schreiben dort:</p>

<blockquote><p>Auf Basis gesetzlicher Bestimmungen haben wir die Adressdaten der
Einwohnermeldeämter mit den bei uns angemeldeten Beitragszahlern
abgeglichen. Unter Ihrem Namen konnten wir für diese Wohnung kein
Beitragskonto finden.</p></blockquote>

<p>Oh! Das ist interessant: Sie kannten meinen Namen und Anschrift,
schafften es aber dennoch nicht, das mit dem damaligen Vorgang
zusammenzubringen? Recht schwaches Bild für so eine Institution, die
doch so professionell mit Adressdaten umzugehen weiß.</p>

<p>Ich beschloss, den Beitragsservice etwas zappeln zu lassen. Meine
Antwort vom <strong>24. April 2014</strong> war daher entsprechend kryptisch:</p>

<blockquote><p>Ich verweise auf mein Schreiben vom 24. April 2012 an die
seinerzeitige Gebühreneinzugszentrale, die Antwort hierauf vom
10. Mai 2012 und mein Schreiben vom 16. Mai 2012 mit dem
Aktenzeichen (&hellip;). Am seinerzeitigen Sacheverhalt hat sich nichts
geändert. Demgemäß wird ein Rundfunkbeitrag für die von mir bewohnte
Wohnung entrichtet und der Rundfunkbeitrag wird auch regelmäßig
abgebucht.</p></blockquote>

<p>Nun war ich gespannt, ob sie es mit etwas Mühe nicht doch hinbekommen,
den Vorgang von damals wieder zu finden und ihren aktuellen Vorgang
abzuschließen. Nach der Beitragsservice-Bedenkzeit von acht Wochen
(ich meine mich zu erinnern, dass die GEZ es meistens in sechs Wochen
geschafft hatte, eine Antwort zu schreiben), kam nun mit Schreiben vom
<strong>26. Juni 2014</strong> eine erneute Bitte um Mitteilung der Beitragsnummer:</p>

<blockquote><p>Gerne bearbeiten wir Ihre Angaben. Hierfür benötigen wir jedoch noch
die Beitragsnummer, unter der die Wohnung angemeldet ist. Teilen Sie
uns diese bitte auf dem Antwortbogen mit.</p></blockquote>

<p>Alternativ können sie das wohl auch aus der Bankverbindung, früheren
Anschriften, früheren Namen etc. herausfinden. Warum jedoch nicht über
meine Anschrift und mein damaliges Schreiben? Ach ja, die üblichen
Daumenschrauben wurden dem Schreiben noch hinzugefügt:</p>

<blockquote><p>Sollten Sie uns nicht innerhalb der Frist (von vier Wochen, D.B.)
gehen wir davon aus, dass eine Anmeldung für diese Wohnung
erforderlich ist.</p></blockquote>

<p>Ahja, der Beitragsservice braucht acht Wochen um nach der
Beitragsnummer zu fragen, ich soll aber in vier Wochen antworten?</p>

<p>Nun ging mit das Porto zu sehr ins Geld und ich wechselte auf
E-Mails. Am <strong>7. Juli 2014</strong> teilte ich dem Beitragsservice noch mal alles
mit: Neue Wohnung, neue Beitragsnummer etc. Und dass Sie das ja alles
seit dem Jahr 2012 schon wüssten. Ich ermunterte den Beitragsservice,
doch einfach noch mal einen Adressabgleich durchzuführen:</p>

<blockquote><p>Ich bin daher etwas verwundert, dass ich nun schon wieder über einen bei
Ihnen bekannten und aktenkundigen Sachverhalt erneut Auskunft erteilen
soll. Denn die seinerzeit mitgeteilten Daten mussten doch irgendwie mir
und meiner Anschrift (die sich ja auch nicht geändert hat) zugeordnet
worden sein.
Ich bin zuversichtlich, es ist Ihnen ein Leichtes, aufgrund obiger
Informationen und Ihnen bereits vor zwei Jahren mitgeteilten
Sachverhaltes Ihren Vorgang abschließen zu können.</p></blockquote>

<p>Nun ging es Schlag auf Schlag, denn völlig unerwartet antwortete der
Beitragsservice bereits am <strong>22. Juli 2014</strong>. Und nun das erste
Eingeständnis:</p>

<blockquote><p>Sie teilen mit, dass Sie bereits in 05.2012 mitgeteilt haben, (&hellip;)
Dieses Schreiben liegt uns leider nicht mehr vor, so dass wir auf
dessen Inhalt nicht zurückgreifen können.</p></blockquote>

<p>Wie kann dies sein? Es handelt sich doch um eine Verwaltung? Die
sollte doch ihre Aktenführung in Ordnung halten können. In meiner
E-Mail vom <strong>28. Juli 2014</strong> erlöste ich den Beitragsservice und teilte
ihnen die Beitragsnummer mit. Allerdings nicht ohne noch ein wenig
meine Besorgnis über die Aktenführung zum Ausdruck zu bringen:</p>

<blockquote><p>Des Weiteren sehen Sie mich überrascht, dass Sie auf ein Schreiben
aus dem Jahr 2012 keinen Zugriff mehr haben. Da ich bisher davon
ausging, dass sich die öffentlich-rechtliche Verwaltung in
besonderem Maße durch die Schriftlichkeit und Aktenkundigkeit Ihres
Handelns auszeichnet, ging ich davon aus, dass zu mir auch die
Unterlagen und der Schriftverkehr verfügbar sind. Ich bitte Sie
höflichst mir mitzuteilen, wie dieses Schriftgut verloren gegangen
ist und wie sichergestellt ist, dass nicht alle zwei Jahre erneut
eine Anfrage wie die vom 17.04.2014 (&hellip;) bei mir eingeht.</p></blockquote>

<p>Nach vier Wochen erreichte mich dann die Auskunft am <strong>27. August
2014</strong>:</p>

<blockquote><p>Sie fragen, warum wir keinen Zugang zu Ihrem Schreiben vom
24.04.2012 hatten. Der Schriftverkehr wurde unter Ihrem abgemeldeten
Beitragskonto (&hellip;) sowie unter dem Beitragskonto (&hellip;) verfilmt. Da
Sie nun den Bezug zur Beitragsnummer (&hellip;) hergestellt haben, können
wir den Schriftverkehr einsehen. Wir hoffen, Ihnen mit diesen
Erläuterungen weitergeholfen zu haben.</p></blockquote>

<p>Ohja, das haben sie. Nachdem der Vorgang wohl beiderseits genügend
Schmerzen verursacht hat, habe ich es dabei bewenden lassen.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mind the storage driver for Ubuntu cloud images (on Azure)]]></title>
    <link href="http://www.dbrunner.de/blog/2014/07/24/mind-the-storage-driver-for-ubuntu-cloud-images-on-azure/"/>
    <updated>2014-07-24T12:30:43+00:00</updated>
    <id>http://www.dbrunner.de/blog/2014/07/24/mind-the-storage-driver-for-ubuntu-cloud-images-on-azure</id>
    <content type="html"><![CDATA[<p>A few days ago I wanted to build Firefox OS&#8217; newest release for a
friend. Because I did not wanted these GB of code, binaries etc. on my
notebook I fired up an Ubuntu image on Microsoft Azure. I feared that
at a certain point in the build process I may had to download
everything to my local machine and therefore I installed Docker via a
simple</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo apt-get install docker.io</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>Then I started the build process as laid out on Mozilla&rsquo;s Developer
Network. But, during downloading the source code (that&rsquo;s about 12 GB
of Git repositories from Mozilla and Android), I got a &ldquo;no more space
left on device&rdquo;. That was strange: I had a 100 GB volume attached to
the VM and enough space and inodes left. After some searching I asked
on the IRC channel and got a good hint: &ldquo;What&rsquo;s your storage driver?&rdquo;</p>

<p>Well, I thought that it&rsquo;s AUFS; I wanted to add &ldquo;as usual&rdquo; because
AUFS was available on my notebook from the beginning. But a <code>docker.io
info</code> gave me:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo docker.io info
</span><span class='line'>Containers: 0
</span><span class='line'>Images: 0
</span><span class='line'>Storage Driver: devicemapper
</span><span class='line'> Pool Name: docker-8:1-131188-pool
</span><span class='line'> Data file: /var/lib/docker/devicemapper/devicemapper/data
</span><span class='line'> Metadata file: /var/lib/docker/devicemapper/devicemapper/metadata
</span><span class='line'> Data Space Used: 291.5 Mb
</span><span class='line'> Data Space Total: 102400.0 Mb
</span><span class='line'> Metadata Space Used: 0.7 Mb
</span><span class='line'> Metadata Space Total: 2048.0 Mb
</span><span class='line'>Execution Driver: native-0.1
</span><span class='line'>Kernel Version: 3.13.0-29-generic
</span><span class='line'>WARNING: No swap limit support</span></code></pre></td></tr></table></div></figure>


<p>I then learned that somehow the DeviceMapper driver only allows a
certain amount of diffs and I reached that amount with my build
process. (Maybe it&rsquo;s possible to relax that restriction but I do not
know how.)</p>

<p>I learned as well that the Ubuntu cloud image that is provided by
Microsoft Azure doesn&rsquo;t have AUFS support. Therefore Docker uses the
DeviceMapper storage driver instead. After I installed the AUFS
support I could export the images, change the storage driver and
import the images again.</p>

<p>It would be nice seeing the Docker documentation being more detailed
on those storage drivers.</p>

<p><strong>(Update 2014-10-23)</strong> Thanks to
 <a href="http://blog.iron.io/2014/10/docker-in-production-what-weve-learned.html">this blog post from Iron.io</a>
 I found some documentation of the devicemapper storage driver. It is
 located in the
 <a href="https://github.com/docker/docker/tree/master/daemon/graphdriver/devmapper">Repository</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DateTime conversion can be tricky]]></title>
    <link href="http://www.dbrunner.de/blog/2014/07/24/datetime-conversion-can-be-tricky/"/>
    <updated>2014-07-24T09:41:36+00:00</updated>
    <id>http://www.dbrunner.de/blog/2014/07/24/datetime-conversion-can-be-tricky</id>
    <content type="html"><![CDATA[<p>I wrote a small Lisp application and a JavaScript client gets some
data from that application. All time stamps are returned as &ldquo;Lisp&rdquo;
time stamps, i.e. an integer with seconds where zero equals Jan 01
1900.</p>

<p>In the JS client the time stamp is then converted to JS time stamps,
i.e. millisconds where zero equals Jan 01 1970.</p>

<p>When testing the application I noticed that sometimes the displayed
date is one day behind. For example in the data base I have Jan 05
1980 but in JavaScript I get a Jan 04 1980. But some other dates
worked: A time stamp Jan 05 1970 was correctly converted to Jan 05
1970.</p>

<p>I had a look into the JavaScript code and found:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>convA = function(ts) {
</span><span class='line'>  tmp = new Date(ts*1000);
</span><span class='line'>  tmp.setFullYear(tmp.getFullYear() - 70);
</span><span class='line'>  return tmp.getTime();
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>It&rsquo;s likely the developer thought: &ldquo;Well, it&rsquo;s millisecond instead of
second. Therefore I multiply by 1,000. But then I am 70 years in the
future and I have to substract 70 years and everything will be ok.&rdquo;</p>

<p>After thinking a while I came to the conclusion: Of course not!</p>

<p>The developer made the assumption that there are as many leap years
between 1900 and 1970 as between <code>ts</code> and <code>ts+70</code>. Obviously that
assumption does not hold for all time stamps. And therefore sometimes
the resulting JavaScript date is one day behind.</p>

<p>So a better solution would be to substract all seconds between 1900
and 1970 from <code>ts</code>, multiply by 1,000 and treat this as a JavaScript
time stamp. Perhaps best would be to do the conversion in the Lisp
process and only deliver a JavaScript-like time stamp.</p>
]]></content>
  </entry>
  
</feed>
